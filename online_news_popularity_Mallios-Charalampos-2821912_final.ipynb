{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online News Popularity Data Set\n",
    "\n",
    "\n",
    "> Mallios Charalampos, Student - p2821912 <br />\n",
    "> Department of Management Science and Technology <br />\n",
    "> Msc Business Analytics <br />\n",
    "> Athens University of Economics and Business <br />\n",
    "> p2821912@aueb.gr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First step was to initalize Spark and set memory boundaries.\n",
    "* Then we created the schema, according to the data description and type and read the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import SparkConf\n",
    "#initialize Spakr session\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "# create app and set memory range in the executors and driver in order not to overspend it.\n",
    "spark =  SparkSession.builder.appName(\"online-popularity-news\").config(\"spark.executor.memory\", \"6g\").config(\"spark.driver.memory\", \"5g\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- url: string (nullable = true)\n",
      " |-- timedelta: double (nullable = true)\n",
      " |-- n_tokens_title: double (nullable = true)\n",
      " |-- n_tokens_content: double (nullable = true)\n",
      " |-- n_unique_tokens: double (nullable = true)\n",
      " |-- n_non_stop_words: double (nullable = true)\n",
      " |-- n_non_stop_unique_tokens: double (nullable = true)\n",
      " |-- num_hrefs: double (nullable = true)\n",
      " |-- num_self_hrefs: double (nullable = true)\n",
      " |-- num_imgs: double (nullable = true)\n",
      " |-- num_videos: double (nullable = true)\n",
      " |-- average_token_length: double (nullable = true)\n",
      " |-- num_keywords: double (nullable = true)\n",
      " |-- data_channel_is_lifestyle: double (nullable = true)\n",
      " |-- data_channel_is_entertainment: double (nullable = true)\n",
      " |-- data_channel_is_bus: double (nullable = true)\n",
      " |-- data_channel_is_socmed: double (nullable = true)\n",
      " |-- data_channel_is_tech: double (nullable = true)\n",
      " |-- data_channel_is_world: double (nullable = true)\n",
      " |-- kw_min_min: double (nullable = true)\n",
      " |-- kw_max_min: double (nullable = true)\n",
      " |-- kw_avg_min: double (nullable = true)\n",
      " |-- kw_min_max: double (nullable = true)\n",
      " |-- kw_max_max: double (nullable = true)\n",
      " |-- kw_avg_max: double (nullable = true)\n",
      " |-- kw_min_avg: double (nullable = true)\n",
      " |-- kw_max_avg: double (nullable = true)\n",
      " |-- kw_avg_avg: double (nullable = true)\n",
      " |-- self_reference_min_shares: double (nullable = true)\n",
      " |-- self_reference_max_shares: double (nullable = true)\n",
      " |-- self_reference_avg_sharess: double (nullable = true)\n",
      " |-- weekday_is_monday: double (nullable = true)\n",
      " |-- weekday_is_tuesday: double (nullable = true)\n",
      " |-- weekday_is_wednesday: double (nullable = true)\n",
      " |-- weekday_is_thursday: double (nullable = true)\n",
      " |-- weekday_is_friday: double (nullable = true)\n",
      " |-- weekday_is_saturday: double (nullable = true)\n",
      " |-- weekday_is_sunday: double (nullable = true)\n",
      " |-- is_weekend: double (nullable = true)\n",
      " |-- LDA_00: double (nullable = true)\n",
      " |-- LDA_01: double (nullable = true)\n",
      " |-- LDA_02: double (nullable = true)\n",
      " |-- LDA_03: double (nullable = true)\n",
      " |-- LDA_04: double (nullable = true)\n",
      " |-- global_sentiment_polarity: double (nullable = true)\n",
      " |-- global_subjectivity: double (nullable = true)\n",
      " |-- global_rate_positive_words: double (nullable = true)\n",
      " |-- global_rate_negative_words: double (nullable = true)\n",
      " |-- rate_positive_words: double (nullable = true)\n",
      " |-- rate_negative_words: double (nullable = true)\n",
      " |-- avg_positive_polarity: double (nullable = true)\n",
      " |-- min_positive_polarity: double (nullable = true)\n",
      " |-- max_positive_polarity: double (nullable = true)\n",
      " |-- avg_negative_polarity: double (nullable = true)\n",
      " |-- min_negative_polarity: double (nullable = true)\n",
      " |-- max_negative_polarity: double (nullable = true)\n",
      " |-- title_subjectivity: double (nullable = true)\n",
      " |-- title_sentiment_polarity: double (nullable = true)\n",
      " |-- abs_title_subjectivity: double (nullable = true)\n",
      " |-- abs_title_sentiment_polarity: double (nullable = true)\n",
      " |-- shares: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>...</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://mashable.com/2013/01/07/amazon-instant-...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>0.663594</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.350000</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>593.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://mashable.com/2013/01/07/ap-samsung-spon...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.604743</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.791946</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.118750</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>711.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://mashable.com/2013/01/07/apple-40-billio...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>0.575130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.466667</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://mashable.com/2013/01/07/astronaut-notre...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>0.503788</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.665635</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.369697</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://mashable.com/2013/01/07/att-u-verse-apps/</td>\n",
       "      <td>731.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>0.415646</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.540890</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.220192</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>505.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  timedelta  \\\n",
       "0  http://mashable.com/2013/01/07/amazon-instant-...      731.0   \n",
       "1  http://mashable.com/2013/01/07/ap-samsung-spon...      731.0   \n",
       "2  http://mashable.com/2013/01/07/apple-40-billio...      731.0   \n",
       "3  http://mashable.com/2013/01/07/astronaut-notre...      731.0   \n",
       "4   http://mashable.com/2013/01/07/att-u-verse-apps/      731.0   \n",
       "\n",
       "   n_tokens_title  n_tokens_content  n_unique_tokens  n_non_stop_words  \\\n",
       "0            12.0             219.0         0.663594               1.0   \n",
       "1             9.0             255.0         0.604743               1.0   \n",
       "2             9.0             211.0         0.575130               1.0   \n",
       "3             9.0             531.0         0.503788               1.0   \n",
       "4            13.0            1072.0         0.415646               1.0   \n",
       "\n",
       "   n_non_stop_unique_tokens  num_hrefs  num_self_hrefs  num_imgs  ...  \\\n",
       "0                  0.815385        4.0             2.0       1.0  ...   \n",
       "1                  0.791946        3.0             1.0       1.0  ...   \n",
       "2                  0.663866        3.0             1.0       1.0  ...   \n",
       "3                  0.665635        9.0             0.0       1.0  ...   \n",
       "4                  0.540890       19.0            19.0      20.0  ...   \n",
       "\n",
       "   min_positive_polarity  max_positive_polarity  avg_negative_polarity  \\\n",
       "0               0.100000                    0.7              -0.350000   \n",
       "1               0.033333                    0.7              -0.118750   \n",
       "2               0.100000                    1.0              -0.466667   \n",
       "3               0.136364                    0.8              -0.369697   \n",
       "4               0.033333                    1.0              -0.220192   \n",
       "\n",
       "   min_negative_polarity  max_negative_polarity  title_subjectivity  \\\n",
       "0                 -0.600              -0.200000            0.500000   \n",
       "1                 -0.125              -0.100000            0.000000   \n",
       "2                 -0.800              -0.133333            0.000000   \n",
       "3                 -0.600              -0.166667            0.000000   \n",
       "4                 -0.500              -0.050000            0.454545   \n",
       "\n",
       "   title_sentiment_polarity  abs_title_subjectivity  \\\n",
       "0                 -0.187500                0.000000   \n",
       "1                  0.000000                0.500000   \n",
       "2                  0.000000                0.500000   \n",
       "3                  0.000000                0.500000   \n",
       "4                  0.136364                0.045455   \n",
       "\n",
       "   abs_title_sentiment_polarity  shares  \n",
       "0                      0.187500   593.0  \n",
       "1                      0.000000   711.0  \n",
       "2                      0.000000  1500.0  \n",
       "3                      0.000000  1200.0  \n",
       "4                      0.136364   505.0  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType\n",
    "\n",
    "#define the schema , for each feature the correct datatype\n",
    "schema = StructType([\n",
    "StructField(\"url\", StringType(), True),\n",
    "StructField(\"timedelta\", DoubleType(), True),\n",
    "StructField(\"n_tokens_title\", DoubleType(), True),\n",
    "StructField(\"n_tokens_content\", DoubleType(), True),\n",
    "StructField(\"n_unique_tokens\", DoubleType(), True),\n",
    "StructField(\"n_non_stop_words\", DoubleType(), True),\n",
    "StructField(\"n_non_stop_unique_tokens\", DoubleType(), True),\n",
    "StructField(\"num_hrefs\", DoubleType(), True),\n",
    "StructField(\"num_self_hrefs\", DoubleType(), True),\n",
    "StructField(\"num_imgs\", DoubleType(), True),\n",
    "StructField(\"num_videos\", DoubleType(), True),\n",
    "StructField(\"average_token_length\", DoubleType(), True),\n",
    "StructField(\"num_keywords\", DoubleType(), True),\n",
    "StructField(\"data_channel_is_lifestyle\", DoubleType(), True),\n",
    "StructField(\"data_channel_is_entertainment\", DoubleType(), True),\n",
    "StructField(\"data_channel_is_bus\", DoubleType(), True),\n",
    "StructField(\"data_channel_is_socmed\", DoubleType(), True),\n",
    "StructField(\"data_channel_is_tech\", DoubleType(), True),\n",
    "StructField(\"data_channel_is_world\", DoubleType(), True),\n",
    "StructField(\"kw_min_min\", DoubleType(), True),\n",
    "StructField(\"kw_max_min\", DoubleType(), True),\n",
    "StructField(\"kw_avg_min\", DoubleType(), True),\n",
    "StructField(\"kw_min_max\", DoubleType(), True),\n",
    "StructField(\"kw_max_max\", DoubleType(), True),\n",
    "StructField(\"kw_avg_max\", DoubleType(), True),\n",
    "StructField(\"kw_min_avg\", DoubleType(), True),\n",
    "StructField(\"kw_max_avg\", DoubleType(), True),\n",
    "StructField(\"kw_avg_avg\", DoubleType(), True),\n",
    "StructField(\"self_reference_min_shares\", DoubleType(), True),\n",
    "StructField(\"self_reference_max_shares\", DoubleType(), True),\n",
    "StructField(\"self_reference_avg_sharess\", DoubleType(), True),\n",
    "StructField(\"weekday_is_monday\", DoubleType(), True),\n",
    "StructField(\"weekday_is_tuesday\", DoubleType(), True),\n",
    "StructField(\"weekday_is_wednesday\", DoubleType(), True),\n",
    "StructField(\"weekday_is_thursday\", DoubleType(), True),\n",
    "StructField(\"weekday_is_friday\", DoubleType(), True),\n",
    "StructField(\"weekday_is_saturday\", DoubleType(), True),\n",
    "StructField(\"weekday_is_sunday\", DoubleType(), True),\n",
    "StructField(\"is_weekend\", DoubleType(), True),\n",
    "StructField(\"LDA_00\", DoubleType(), True),\n",
    "StructField(\"LDA_01\", DoubleType(), True),\n",
    "StructField(\"LDA_02\", DoubleType(), True),\n",
    "StructField(\"LDA_03\", DoubleType(), True),\n",
    "StructField(\"LDA_04\", DoubleType(), True),\n",
    "StructField(\"global_sentiment_polarity\", DoubleType(), True),\n",
    "StructField(\"global_subjectivity\", DoubleType(), True),\n",
    "StructField(\"global_rate_positive_words\", DoubleType(), True),\n",
    "StructField(\"global_rate_negative_words\", DoubleType(), True),\n",
    "StructField(\"rate_positive_words\", DoubleType(), True),\n",
    "StructField(\"rate_negative_words\", DoubleType(), True),\n",
    "StructField(\"avg_positive_polarity\", DoubleType(), True),\n",
    "StructField(\"min_positive_polarity\", DoubleType(), True),\n",
    "StructField(\"max_positive_polarity\", DoubleType(), True),\n",
    "StructField(\"avg_negative_polarity\", DoubleType(), True),\n",
    "StructField(\"min_negative_polarity\", DoubleType(), True),\n",
    "StructField(\"max_negative_polarity\", DoubleType(), True),\n",
    "StructField(\"title_subjectivity\", DoubleType(), True),\n",
    "StructField(\"title_sentiment_polarity\", DoubleType(), True),\n",
    "StructField(\"abs_title_subjectivity\", DoubleType(), True),\n",
    "StructField(\"abs_title_sentiment_polarity\", DoubleType(), True),\n",
    "StructField(\"shares\", DoubleType(), True)])\n",
    "\n",
    "#read the data and stored as they were on schema defined\n",
    "data = spark\\\n",
    "  .read\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .schema(schema)\\\n",
    "  .csv(\"OnlineNewsPopularity.csv\")\n",
    "#see the schema\n",
    "data.printSchema()\n",
    "# see the first 5 rows\n",
    "data.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fields 'url' and 'timedelta' that were not predictive were dropped.\n",
    "* After that the dataset splitted into training and test with 0.75/0.35 ratio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# drop the non - predictive features\n",
    "data = data.drop('url')\n",
    "data = data.drop('timedelta')\n",
    "\n",
    "#split to train and test\n",
    "(training_data, test_data) = data.randomSplit([0.75, 0.25])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We created the vector with all the x features with Vector assembler.\n",
    "* Created the pipeline, define Random Forest Regresson with different tuning parameters.\n",
    "* And after cross validation we choose the best model and test it on test dataset.\n",
    "* Finally RMSE along with R-squared were printed to evaluate the predictions, together with the statistics of Y variable 'shares'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cacheNodeIds: False\n",
      "checkpointInterval: 10\n",
      "featureSubsetStrategy: sqrt\n",
      "featuresCol: features\n",
      "impurity: variance\n",
      "labelCol: shares\n",
      "maxBins: 32\n",
      "maxDepth: 5\n",
      "maxMemoryInMB: 256\n",
      "minInfoGain: 0.0\n",
      "minInstancesPerNode: 1\n",
      "numTrees: 100\n",
      "predictionCol: prediction\n",
      "seed: -3375131582288097643\n",
      "subsamplingRate: 1.0\n",
      "Root Mean Squared Error (RMSE) on test data = 8016.32\n",
      "R2 on test data = 0.0176969\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder,TrainValidationSplit\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "\n",
    "# create the vector features containing the features data all together(x) except Y(shares)\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[ x for x in training_data.columns if x != \"shares\" ],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "\n",
    "\n",
    "#define the random forest regressor\n",
    "rf = RandomForestRegressor().\\\n",
    "    setLabelCol(\"shares\").\\\n",
    "    setFeaturesCol(\"features\").\\\n",
    "    setPredictionCol(\"prediction\")\n",
    "\n",
    "#set different tuning hyperparameters levels for optimization\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "  .addGrid(rf.featureSubsetStrategy, ['log2','sqrt','onethird']) \\\n",
    "  .addGrid(rf.numTrees, [100,500,700]) \\\n",
    "  .build()\n",
    "\n",
    "#define the pipeline contains the steps \n",
    "pipeline = Pipeline().setStages([assembler,rf])\n",
    "\n",
    "#define the evaluator of regresson - based on RMSE\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"shares\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator2 = RegressionEvaluator(\n",
    "    labelCol=\"shares\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "\n",
    "\n",
    "#make Cross validation to tune our regressor best parameters\n",
    "tvs = TrainValidationSplit(estimator=pipeline,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=evaluator,\n",
    "                           # 80% of the data will be used for training, 20% for validation.\n",
    "                           trainRatio=0.8)\n",
    "\n",
    "#fit the model\n",
    "model = tvs.fit(training_data)\n",
    "\n",
    "#make the prediction on test data\n",
    "predictions = model.bestModel.transform(test_data)\n",
    "\n",
    "# extract for the best model selected the tuning hyperparameters\n",
    "params = model.bestModel.stages[-1].extractParamMap()\n",
    "\n",
    "for param, value in params.items():\n",
    "        print(f'{param.name}: {value}')\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "r2 = evaluator2.evaluate(predictions)\n",
    "\n",
    "#extract RMSE and R^2 from our model prediction\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "print(\"R2 on test data = %g\" % r2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|            shares|\n",
      "+-------+------------------+\n",
      "|  count|             39644|\n",
      "|   mean|3395.3801836343455|\n",
      "| stddev| 11626.95074865171|\n",
      "|    min|               1.0|\n",
      "|    max|          843300.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#select shares and get descriptive statistics of target variable\n",
    "shares = data.select('shares')\n",
    "shares.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **RMSE** value (8016.32) is **not** good in terms of predictive performance. The value is lower than Standard Deviation of the shares(Y variable), although is much higher than the mean of target variable 3395. Morevoer, regarding R-squared its performance is also poor (0.017), sice R-squared defines how well the regression predictions approximate the real data points.\n",
    "    \n",
    "* A metric to better evaluate the **RMSE** is **Scatter Index** which is defined as : **RMSE / mean(Y)**. This value must be below < 1 but in our case is much higher 8016/3395(~2.36). \n",
    "\n",
    "* In articles with great number of shares (e.x. > 100.000) our model would be ok in terms of prediction error. Now, with the average shares being around 3395 shares, the 8016 shares error is not acceptable.\n",
    "\n",
    "* We can observe the reason behind that : our dataset is **imbalanced** and the distribution has skewness. The max value of 843300 is far away from the mean (3395) and this has huge impact on our prediction effort , since some **outliers** like the max value have affect our results.\n",
    "\n",
    "* To prove that we get the 99% quantile to explore what is happening :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[31700.0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outliers = data.approxQuantile(\"shares\", [0.99], 0)\n",
    "outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Indeed, we see that 99% of observations are below 31700. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggest a different approach based on Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We should also check for the **correlation** between X variables and Y ('shares').\n",
    "* In a second try to find a better predictive model we should make variable selection based on correlation and keep \n",
    "only those which make impact in a new dataset.\n",
    "* So below we repeat the process again , now having only the high **correlated data from dataset** and making some scaling on data to see if it will help on normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_tokens_title\n",
      "0.008783118841819185\n",
      "n_tokens_content\n",
      "0.002458984345090757\n",
      "n_unique_tokens\n",
      "0.0008063456613076668\n",
      "n_non_stop_words\n",
      "0.00044294161730838635\n",
      "n_non_stop_unique_tokens\n",
      "0.00011417193781090629\n",
      "num_hrefs\n",
      "0.04540400840592801\n",
      "num_self_hrefs\n",
      "-0.0019004033728368833\n",
      "num_imgs\n",
      "0.03938759784762878\n",
      "num_videos\n",
      "0.023936069530130968\n",
      "average_token_length\n",
      "-0.02200729806193876\n",
      "num_keywords\n",
      "0.021818227151875386\n",
      "data_channel_is_lifestyle\n",
      "0.005831267306546816\n",
      "data_channel_is_entertainment\n",
      "-0.017006198429558247\n",
      "data_channel_is_bus\n",
      "-0.012376166187935982\n",
      "data_channel_is_socmed\n",
      "0.005021216306495134\n",
      "data_channel_is_tech\n",
      "-0.013252874428024544\n",
      "data_channel_is_world\n",
      "-0.049497308931198525\n",
      "kw_min_min\n",
      "-0.0010509877309311786\n",
      "kw_max_min\n",
      "0.030113936670928126\n",
      "kw_avg_min\n",
      "0.03040554741568556\n",
      "kw_min_max\n",
      "0.0039014319770320736\n",
      "kw_max_max\n",
      "0.007862569278242268\n",
      "kw_avg_max\n",
      "0.04468584480857034\n",
      "kw_min_avg\n",
      "0.039550694479642406\n",
      "kw_max_avg\n",
      "0.06430586384745443\n",
      "kw_avg_avg\n",
      "0.11041286270954459\n",
      "self_reference_min_shares\n",
      "0.05595757511084421\n",
      "self_reference_max_shares\n",
      "0.04711522330980047\n",
      "self_reference_avg_sharess\n",
      "0.05778889739104393\n",
      "weekday_is_monday\n",
      "0.009726435058827832\n",
      "weekday_is_tuesday\n",
      "-0.007940651946220838\n",
      "weekday_is_wednesday\n",
      "-0.003800671899536126\n",
      "weekday_is_thursday\n",
      "-0.008833243677374618\n",
      "weekday_is_friday\n",
      "-0.0038843499366703147\n",
      "weekday_is_saturday\n",
      "0.01508224937443862\n",
      "weekday_is_sunday\n",
      "0.008229538740752407\n",
      "is_weekend\n",
      "0.01695818525536523\n",
      "LDA_00\n",
      "-0.003793063141475984\n",
      "LDA_01\n",
      "-0.010182873758371989\n",
      "LDA_02\n",
      "-0.059162729837758665\n",
      "LDA_03\n",
      "0.08377149200458589\n",
      "LDA_04\n",
      "-0.016621938194680205\n",
      "global_sentiment_polarity\n",
      "0.03160406219955463\n",
      "global_subjectivity\n",
      "0.004162929062277361\n",
      "global_rate_positive_words\n",
      "0.0005432341490310408\n",
      "global_rate_negative_words\n",
      "0.006615173122292244\n",
      "rate_positive_words\n",
      "-0.013241304213398837\n",
      "rate_negative_words\n",
      "-0.00518313830930748\n",
      "avg_positive_polarity\n",
      "0.012142198867957643\n",
      "min_positive_polarity\n",
      "-4.009526121358077e-05\n",
      "max_positive_polarity\n",
      "0.01006779439658019\n",
      "avg_negative_polarity\n",
      "-0.03202890859561201\n",
      "min_negative_polarity\n",
      "-0.019297375184780127\n",
      "max_negative_polarity\n",
      "-0.019300211388182775\n",
      "title_subjectivity\n",
      "0.021966682303944024\n",
      "title_sentiment_polarity\n",
      "0.012771873141514574\n",
      "abs_title_subjectivity\n",
      "0.0014809979007599867\n",
      "abs_title_sentiment_polarity\n",
      "0.027135230985338103\n",
      "shares\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "\n",
    "data = data.drop('url')\n",
    "data = data.drop('timedelta')\n",
    "#select all columns to keep the names\n",
    "correlat = data.select('*')\n",
    "\n",
    "#enumerate on column names and print the correlation with Y\n",
    "names = correlat.schema.names\n",
    "for name in names:\n",
    "  print(name)\n",
    "  print(correlat.stat.corr(name,\"shares\"))\n",
    "\n",
    "#The newdataset contains only high correlated variables    \n",
    "new = correlat.select('num_hrefs','num_imgs','data_channel_is_world','kw_avg_max','kw_min_avg','kw_max_avg','kw_avg_avg','self_reference_min_shares','self_reference_avg_sharess','LDA_02','LDA_03','shares')\n",
    "\n",
    "#split to train and test\n",
    "(training_data, test_data) = new.randomSplit([0.75, 0.25])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>data_channel_is_world</th>\n",
       "      <th>kw_avg_max</th>\n",
       "      <th>kw_min_avg</th>\n",
       "      <th>kw_max_avg</th>\n",
       "      <th>kw_avg_avg</th>\n",
       "      <th>self_reference_min_shares</th>\n",
       "      <th>self_reference_avg_sharess</th>\n",
       "      <th>LDA_02</th>\n",
       "      <th>LDA_03</th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16080.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2939.239130</td>\n",
       "      <td>1596.149445</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020019</td>\n",
       "      <td>0.020554</td>\n",
       "      <td>1700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17255.555556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4380.000000</td>\n",
       "      <td>2328.023284</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.260606</td>\n",
       "      <td>0.023187</td>\n",
       "      <td>4500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17628.000000</td>\n",
       "      <td>440.0</td>\n",
       "      <td>4423.333333</td>\n",
       "      <td>2305.412167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020075</td>\n",
       "      <td>0.918569</td>\n",
       "      <td>1000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18180.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3169.658537</td>\n",
       "      <td>2035.970551</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020007</td>\n",
       "      <td>0.020177</td>\n",
       "      <td>1500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18442.857143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2810.766129</td>\n",
       "      <td>1415.501897</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028589</td>\n",
       "      <td>0.660815</td>\n",
       "      <td>589.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_hrefs  num_imgs  data_channel_is_world    kw_avg_max  kw_min_avg  \\\n",
       "0        0.0       0.0                    0.0  16080.000000         0.0   \n",
       "1        0.0       0.0                    0.0  17255.555556         0.0   \n",
       "2        0.0       0.0                    0.0  17628.000000       440.0   \n",
       "3        0.0       0.0                    0.0  18180.000000         0.0   \n",
       "4        0.0       0.0                    0.0  18442.857143         0.0   \n",
       "\n",
       "    kw_max_avg   kw_avg_avg  self_reference_min_shares  \\\n",
       "0  2939.239130  1596.149445                        0.0   \n",
       "1  4380.000000  2328.023284                        0.0   \n",
       "2  4423.333333  2305.412167                        0.0   \n",
       "3  3169.658537  2035.970551                        0.0   \n",
       "4  2810.766129  1415.501897                        0.0   \n",
       "\n",
       "   self_reference_avg_sharess    LDA_02    LDA_03  shares  \n",
       "0                         0.0  0.020019  0.020554  1700.0  \n",
       "1                         0.0  0.260606  0.023187  4500.0  \n",
       "2                         0.0  0.020075  0.918569  1000.0  \n",
       "3                         0.0  0.020007  0.020177  1500.0  \n",
       "4                         0.0  0.028589  0.660815   589.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if they are stacked correctly\n",
    "training_data.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cacheNodeIds: False\n",
      "checkpointInterval: 10\n",
      "featureSubsetStrategy: log2\n",
      "featuresCol: features\n",
      "impurity: variance\n",
      "labelCol: shares\n",
      "maxBins: 32\n",
      "maxDepth: 5\n",
      "maxMemoryInMB: 256\n",
      "minInfoGain: 0.0\n",
      "minInstancesPerNode: 1\n",
      "numTrees: 500\n",
      "predictionCol: prediction\n",
      "seed: -3375131582288097643\n",
      "subsamplingRate: 1.0\n",
      "Root Mean Squared Error (RMSE) on test data = 10582.4\n",
      "R2 on test data = 0.021219\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder,TrainValidationSplit\n",
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "\n",
    "# making some scaling on data to see if it helps\n",
    "scaler = StandardScaler(inputCol=\"features_ass\", outputCol=\"features\",\n",
    "                        withStd=True, withMean=True)\n",
    "#vector assembler again\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[ x for x in training_data.columns if x != \"shares\" ],\n",
    "    outputCol=\"features_ass\")\n",
    "\n",
    "#define random forest regressor\n",
    "rf = RandomForestRegressor().\\\n",
    "    setLabelCol(\"shares\").\\\n",
    "    setFeaturesCol(\"features\").\\\n",
    "    setPredictionCol(\"prediction\")\n",
    "\n",
    "# parameters tuning and their ranges\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "  .addGrid(rf.featureSubsetStrategy, ['log2','sqrt','onethird']) \\\n",
    "  .addGrid(rf.numTrees, [100,500,700]) \\\n",
    ".build()\n",
    "\n",
    "#set the pipeline with the 3 stages -> assemberl, scaler, rf regressor\n",
    "pipeline = Pipeline().setStages([assembler,scaler,rf])\n",
    "\n",
    "#evaluate the results in terms of RMSE\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"shares\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "#make Cross validation to tune our regressor best parameters\n",
    "tvs = TrainValidationSplit(estimator=pipeline,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=evaluator,\n",
    "                           # 80% of the data will be used for training, 20% for validation.\n",
    "                           trainRatio=0.8)\n",
    "#fit on training data\n",
    "model = tvs.fit(training_data)\n",
    "\n",
    "#apply the best model on test data\n",
    "predictions = model.bestModel.transform(test_data)\n",
    "\n",
    "\n",
    "# extract for the best model selected the tuning hyperparameters\n",
    "params = model.bestModel.stages[-1].extractParamMap()\n",
    "\n",
    "for param, value in params.items():\n",
    "        print(f'{param.name}: {value}')\n",
    "\n",
    "        \n",
    "#extract RMSE and R^2 from our model prediction\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "r2 = evaluator2.evaluate(predictions)\n",
    "\n",
    "\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "print(\"R2 on test data = %g\" % r2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We see that the results were slightly different. **RMSE** was decreased to 10582.4 and **R-squared** was slightly increased to 0.021 , however still in not such an effective way.\n",
    "\n",
    "* So, beside the fact we got a different approach to variable selection the results are not better in terms of predictibe ability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In a nutshell**, we can see that the Random Forest Regressor has not good predictive ability on our dataset. \n",
    "The main reason behind that, is the dataset. The data are inbalanced and the distribution of shares is not normal.\n",
    "Moreover,after applying correlation between every single feauture and Y we saw that there is only some feautures with correlation,but not in such an attentive way , in particarly with absolute correlation above or below 30. So, based on this datset and data distribution we can't expect a better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
